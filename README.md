=======
# ML-Demo ðŸš€

Ein kleines Fullstack-Demo-Projekt, das verschiedene moderne Techniken im Bereich Machine Learning und Webentwicklung kombiniert. Ziel ist es, ein interaktives Interface zur VerfÃ¼gung zu stellen, das ML-FunktionalitÃ¤ten demonstriert â€“ inklusive Entscheidungsbaum, Bildklassifikation und Chatbot.

## âœ¨ Features

- ðŸ§  **RentennÃ¤he-Vorhersage** mit einem Decision Tree (scikit-learn)
- ðŸ±ðŸ¶ **Bildklassifikation** (Hund oder Katze)
- ðŸ¤– **Chatbot** basierend auf Ollama (lokales LLM)
- ðŸŒ **Frontend mit React + TailwindCSS**
- âš™ï¸ **FastAPI-Backend** mit strukturierter API
- ðŸ³ **Docker-ready** (optional)
- ðŸ” CORS-Konfiguration und API-SchlÃ¼sselverwaltung
- ðŸ“ Klare Trennung von Routen, Modellen und Logik

## ðŸ—‚ï¸ Projektstruktur

```
ml-demo/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ routes/
â”‚       â”‚   â”œâ”€â”€ predict.py
â”‚       â”‚   â”œâ”€â”€ classify.py
â”‚       â”‚   â””â”€â”€ chat.py
â”‚       â”œâ”€â”€ ml_model/
â”‚       â”‚   â”œâ”€â”€ decision_tree_model.py
â”‚       â”‚   â”œâ”€â”€ image_classifier.py
â”‚       â”‚   â””â”€â”€ chat_model.py
â”‚       â””â”€â”€ models.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/ui/
â”‚       â”‚   â”œâ”€â”€ DecisionBox.tsx
â”‚       â”‚   â”œâ”€â”€ ClassifierBox.tsx
â”‚       â”‚   â””â”€â”€ ChatbotBox.tsx
â”‚       â”œâ”€â”€ MLFrontend.tsx
â”‚       â””â”€â”€ main.tsx
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸš€ Startanleitung

### 1. Backend starten

```bash
cd backend/app
uvicorn main:app --reload
```

### 2. Frontend starten

```bash
cd frontend
npm install
npm run dev
```

### 3. Ollama Modell starten (fÃ¼r Chatbot)

```bash
ollama run llama3
```

Optional: Stelle sicher, dass deine `.env` korrekt ist und deine Modelle geladen wurden.

## ðŸ“¦ Verwendete Technologien

- **Frontend:** React, TypeScript, TailwindCSS, Vite
- **Backend:** FastAPI, scikit-learn, Pillow, requests
- **LLM-Integration:** Ollama (lokales LLM via REST)
- **DevOps:** Docker (optional), Jenkins (optional)
- **Tooling:** Venv, .env, CORS, JSON-API

## ðŸ”’ Hinweise

Die Chatbot-Komponente kann auch mit OpenAI oder OpenRouter betrieben werden. Aus DatenschutzgrÃ¼nden empfiehlt sich Ollama fÃ¼r lokale Szenarien. 
>>>>>>> c0fb17ce (ml-demo Version 1.0)
