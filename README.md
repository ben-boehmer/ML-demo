=======
# ML-Demo ðŸš€

Ein kleines Fullstack-Demo-Projekt, das verschiedene moderne Techniken im Bereich Machine Learning und Webentwicklung kombiniert. Ziel ist es, ein interaktives Interface zur VerfÃ¼gung zu stellen, das ML-FunktionalitÃ¤ten demonstriert â€“ inklusive Entscheidungsbaum, Bildklassifikation und Chatbot.

## âœ¨ Features

- ðŸ§  **RentennÃ¤he-Vorhersage** mit einem Decision Tree (scikit-learn)
- ðŸ±ðŸ¶ **Bildklassifikation** (Hund oder Katze)
- ðŸ¤– **Chatbot** basierend auf Ollama (lokales LLM)
- ðŸŒ **Frontend mit React + TailwindCSS**
- âš™ï¸ **FastAPI-Backend** mit strukturierter API
- ðŸ³ **Docker-ready** 
- ðŸ” CORS-Konfiguration und API-SchlÃ¼sselverwaltung
- ðŸ“ Klare Trennung von Routen, Modellen und Logik
- ðŸ§ª **CI/CD-Integration** via Jenkinsfile 
- ðŸ“¦ **Deployment per Ansible** 

## ðŸ“¦ Verwendete Technologien

### ðŸŽ¨ Frontend
- **React** mit **TypeScript**
- **TailwindCSS** fÃ¼r schlankes, flexibles Styling
- **Vite** als moderner Build- und Entwicklungsserver

### ðŸ§  Backend & Machine Learning
- **FastAPI** als schnelles, dokumentiertes REST-Framework
- **PyTest** fÃ¼r umfangreiche automatisierbare Tests
- **scikit-learn** fÃ¼r klassische ML-Modelle (z.â€¯B. Entscheidungsbaum)
- **Pillow** fÃ¼r Bildverarbeitung (z.â€¯B. Klassifikation Hund/Katze)
- **requests** fÃ¼r interne HTTP-Aufrufe
- **tensorflow2** fÃ¼r Klassifikation mit einem Convoluted Neural Network

### ðŸ¤– LLM-Integration
- **Ollama** zur lokalen AusfÃ¼hrung eines LLM Ã¼ber REST-Schnittstelle (z.â€¯B. LLaMA)

### âš™ï¸ DevOps & Infrastruktur
- **Docker & Docker Compose** zur Containerisierung (optional, empfohlen)
- **Ansible** zur automatisierten Bereitstellung per Playbook
- **Jenkins** als CI/CD-Option (optional)
- **pyenv** zur Python-Versionierung (z.â€¯B. Python 3.10 gezielt einsetzbar)

### ðŸ§° Tooling & Projektstruktur
- **venv** fÃ¼r virtuelle Python-Umgebung
- **.env-Dateien** zur Trennung von Konfiguration und Code
- **CORS-Konfiguration** fÃ¼r sichere Browser-API-Kommunikation
- **OpenAPI/Swagger** automatisch generiert durch FastAPI
- **JSON-API** fÃ¼r strukturierte, verstÃ¤ndliche Datenschnittstellen

Die Chatbot-Komponente kann auch mit OpenAI oder OpenRouter betrieben werden. Aus DatenschutzgrÃ¼nden empfiehlt sich Ollama fÃ¼r lokale Szenarien. 




## ðŸ—‚ï¸ Projektstruktur

```
ml-demo
â”œâ”€â”€ ansible
â”‚   â”œâ”€â”€ deploy.yml
â”‚   â””â”€â”€ inventory.ini # 
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ app
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ tests
â”‚   â”‚   â”œâ”€â”€ test_chat.py
â”‚   â”‚   â”œâ”€â”€ test_classifiy.py
â”‚   â”‚   â””â”€â”€ test_predict.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ backend@tmp
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ dist
â”‚   â”œâ”€â”€ node_modules
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”œâ”€â”€ App_deaktiviert.tsx
â”‚   â”‚   â”œâ”€â”€ MLFrontend.tsx
â”‚   â”‚   â”œâ”€â”€ main.tsx
â”‚   â”‚   â””â”€â”€ setupTests.ts
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vitest.config.ts
â”œâ”€â”€ jenkins
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ Jenkinsfile
â”œâ”€â”€ Modelfile
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml
```

## ðŸš€ Startanleitung

Alles in jeweils einer eigenen Konsole:

### 1. Backend starten

```bash
cd backend/app
pip install -r requirements.txt
uvicorn app.main:app --reload
```

### 2. Frontend starten

```bash
cd frontend
npm install
npm run dev
```

### 3. Ollama Modell starten (fÃ¼r Chatbot)

```bash
ollama serve
ollama run llama3
```
Anmerkung: Run llama3 kann sehr lange dauern, da ein ganzes LLM heruntergeladen
wird. Ca 7gb kann ich nicht im GitHub Repository speichern.
### 4 Alternative - Docker Compose:

```bash
cd ml-demo/
docker compose up --build -d
```
Achtung: Build des Image kann sehr lange dauern, da Ollama  ~7gb groÃŸ ist.
Es ist nicht best practise das Modell in das Image zu integrieren - aber ich
will eine Out-of-the-Box LÃ¶sung haben und dafÃ¼r ging es sonst nicht anders.

Ansonsten sind auch die nÃ¶tigen Jenkinsfile und in /ansible die deploy.yml
und inventory.ini zu finden.

>>>>>>> c0fb17ce (ml-demo Version 1.0)

